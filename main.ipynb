{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR2-f8cHO1O8"
      },
      "outputs": [],
      "source": [
        "# ---------------- INSTALL DEPENDENCIES ----------------\n",
        "!pip install google-generativeai PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:395: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:395: SyntaxWarning: invalid escape sequence '\\h'\n",
            "/tmp/ipython-input-2246884966.py:395: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  *   **Optimizer:** The Adam optimization rule was selected for its ability to handle sparse gradients and its adaptive learning rate. This involves calculating moving averages of gradients ($m_t$) and squared gradients ($v_t$), followed by bias correction ($\\hat{m}_t, \\hat{v}_t$). The weights are updated using a learning rate ($\\alpha$), the bias-corrected moving average of gradients, and the square root of the bias-corrected moving average of squared gradients (with a small constant $\\epsilon$ for numerical stability). Hyperparameters $\\beta_1$ and $\\beta_2$ control the exponential decay rates for the moving averages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Extracting text from PDF...\n",
            "🧠 Asking Gemini to extract Methodology...\n",
            "⚠️ Could not extract text from Gemini response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "⚠️ Could not extract text from Gemini response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "⚠️ Could not extract text from Gemini response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "⚠️ Could not extract text from Gemini response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "⚠️ Could not extract text from Gemini response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "⚠️ Could not extract text from Gemini response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "✅ Methodology extracted (saved to /content/extracted_methodology.txt)\n",
            "📦 Extracting dataset zip...\n",
            "🔎 Scanning dataset...\n",
            "Found 15879 files. Types: {'images': 15879, 'audio': 0, 'text': 0, 'others': 0}\n",
            "🛠️ Generating preprocessing script via Gemini (iterative)...\n",
            "\n",
            "--- Gemini generation attempt 1/10 ---\n",
            ">> Running generated code...\n",
            "✅ Preprocessing finished. Final dataset info saved to /content/final_dataset_info.json\n",
            "🏗️ Generating model & training script via Gemini (iterative)...\n",
            "\n",
            "--- Gemini generation attempt 1/10 ---\n",
            ">> Running generated code...\n",
            "🔚 Pipeline finished. success = True\n",
            "\n",
            "--- Pipeline summary ---\n",
            "Methodology length: 7529\n",
            "Final dataset files: None\n",
            "Preprocessing script saved to: /content/preprocessing_generated.py\n",
            "Model script saved to: /content/model_generated.py\n",
            "Result metadata saved to: /content/pipeline_result.json\n"
          ]
        }
      ],
      "source": [
        "# full_pipeline_gemini.py\n",
        "# Full pipeline: PDF -> extract Methodology -> dataset zip -> scan -> iterative preprocessing -> re-scan -> model generation & 2-epoch training\n",
        "# Use in Colab: upload your PDF and optional dataset.zip to /content and update paths below.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import zipfile\n",
        "import shutil\n",
        "import traceback\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import random\n",
        "import time\n",
        "\n",
        "import PyPDF2\n",
        "import google.generativeai as genai\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "# Replace with your Gemini API key OR set environment variable GEMINI_API_KEY\n",
        "GEMINI_API_KEY = \"YOUR GEMINI API KEY HERE\"\n",
        "if GEMINI_API_KEY in (None, \"\", \"YOUR_API_KEY_HERE\"):\n",
        "    print(\"⚠️ Please set GEMINI_API_KEY environment variable or replace GEMINI_API_KEY in the script.\")\n",
        "    # Do not exit; user might replace later. But warn.\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "MODEL = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Paths (change as needed)\n",
        "DEFAULT_EXTRACT_DIR = \"/content/dataset_extracted\"\n",
        "PREPROCESS_OUTPUT_JSON = \"/content/dataset_info_after_preprocessing.json\"\n",
        "TRAINING_METADATA_JSON = \"/content/training_metadata.json\"\n",
        "PREPROCESSING_SCRIPT_PATH = \"/content/preprocessing_generated.py\"\n",
        "MODEL_SCRIPT_PATH = \"/content/model_generated.py\"\n",
        "\n",
        "# Threshold above which we sample exactly 10% of files\n",
        "LARGE_DATASET_THRESHOLD = 1000\n",
        "\n",
        "# Max attempts for iterative generation\n",
        "MAX_ATTEMPTS_PREPROCESS = 10\n",
        "MAX_ATTEMPTS_MODEL = 10\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "\n",
        "def safe_get_text(response):\n",
        "    \"\"\"Safely extract text from Gemini response, or return '' if empty.\"\"\"\n",
        "    try:\n",
        "        if hasattr(response, \"text\") and response.text:\n",
        "            return response.text.strip()\n",
        "        elif hasattr(response, \"candidates\") and response.candidates:\n",
        "            for cand in response.candidates:\n",
        "                if cand.content.parts:\n",
        "                    return \"\".join(p.text for p in cand.content.parts if hasattr(p, \"text\"))\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Could not extract text from Gemini response:\", e)\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def safe_write(path, text):\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "def read_text(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# ---------- PDF extraction ----------\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def extract_section_via_gemini(paper_text, section_name, chunk_size=7500):\n",
        "    \"\"\"\n",
        "    Chunk the paper_text and ask Gemini to return ONLY the requested section.\n",
        "    Concatenate results from chunks.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for i in range(0, len(paper_text), chunk_size):\n",
        "        chunk = paper_text[i:i+chunk_size]\n",
        "        prompt = f\"\"\"\n",
        "You are an assistant. From the following research paper text chunk, extract ONLY the '{section_name}' section (if present).\n",
        "If the section continues across chunks, return the available portion from this chunk.\n",
        "Return ONLY the section text — no commentary.\n",
        "\n",
        "Paper chunk:\n",
        "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "        resp = MODEL.generate_content(prompt)\n",
        "        txt = safe_get_text(resp)\n",
        "        if txt:\n",
        "            parts.append(txt)\n",
        "    return \"\\n\".join(parts).strip()\n",
        "\n",
        "def extract_methodology(paper_text):\n",
        "    # Try multiple section names\n",
        "    for name in [\"Methodology\", \"Methods\", \"Method\", \"Experimental Setup\"]:\n",
        "        sec = extract_section_via_gemini(paper_text, name)\n",
        "        if sec and len(sec) > 20:\n",
        "            return sec\n",
        "    # fallback: ask Gemini to extract the part that *looks like* methodology\n",
        "    prompt = f\"\"\"\n",
        "You are an assistant. From the following research paper text, find and return ONLY the portion that explains how the research was conducted (algorithms, architectures, datasets, experimental procedure).\n",
        "Return the text ONLY (no commentary).\n",
        "\n",
        "Paper text:\n",
        "\\\"\\\"\\\"{paper_text[:24000]}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    resp = MODEL.generate_content(prompt)\n",
        "    return safe_get_text(resp)\n",
        "\n",
        "def extract_dataset_names_from_paper(paper_text):\n",
        "    prompt = f\"\"\"\n",
        "From the following research paper text, list the names of datasets used (e.g., MNIST, CIFAR-10, COCO, ImageNet, IMDB).\n",
        "Return only dataset names separated by commas, or 'None' if none are mentioned.\n",
        "\n",
        "Paper text:\n",
        "\\\"\\\"\\\"{paper_text[:8000]}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    resp = MODEL.generate_content(prompt)\n",
        "    txt = safe_get_text(resp)\n",
        "    if not txt or txt.lower().startswith(\"none\"):\n",
        "        return None\n",
        "    # take the first line/list\n",
        "    items = [s.strip() for s in re_split_commas_and_newlines(txt) if s.strip()]\n",
        "    return items\n",
        "\n",
        "def re_split_commas_and_newlines(s):\n",
        "    # helper simple splitter preserving items\n",
        "    parts = []\n",
        "    for line in s.splitlines():\n",
        "        for p in line.split(\",\"):\n",
        "            parts.append(p.strip())\n",
        "    return parts\n",
        "\n",
        "# ---------- Dataset zip handling & scanning ----------\n",
        "\n",
        "def extract_zip(zip_path, extract_to=DEFAULT_EXTRACT_DIR):\n",
        "    if os.path.exists(extract_to):\n",
        "        shutil.rmtree(extract_to)\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_to)\n",
        "    return extract_to\n",
        "\n",
        "def is_image_file(fname):\n",
        "    return fname.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tiff','.gif','.webp'))\n",
        "\n",
        "def is_audio_file(fname):\n",
        "    return fname.lower().endswith(('.wav','.mp3','.flac','.ogg','.m4a','.aac'))\n",
        "\n",
        "def is_text_file(fname):\n",
        "    return fname.lower().endswith(('.txt','.csv','.tsv','.json','.md'))\n",
        "\n",
        "def scan_dataset(root_path, max_preview_files=50):\n",
        "    info = {\n",
        "        \"root\": os.path.abspath(root_path),\n",
        "        \"num_files\": 0,\n",
        "        \"types\": {\"images\":0, \"audio\":0, \"text\":0, \"others\":0},\n",
        "        \"has_train_test_split\": False,\n",
        "        \"train_dir\": None,\n",
        "        \"test_dir\": None,\n",
        "        \"class_folders\": [],\n",
        "        \"sample_files\": {\"images\": [], \"audio\": [], \"text\": [], \"others\": []},\n",
        "        \"total_files_by_ext\": {}\n",
        "    }\n",
        "    # check top-level for train/test\n",
        "    try:\n",
        "        entries = [p.name.lower() for p in Path(root_path).iterdir() if p.is_dir()]\n",
        "    except Exception:\n",
        "        entries = []\n",
        "    if \"train\" in entries or \"test\" in entries:\n",
        "        info[\"has_train_test_split\"] = True\n",
        "        if \"train\" in entries:\n",
        "            info[\"train_dir\"] = os.path.join(root_path, \"train\")\n",
        "        if \"test\" in entries:\n",
        "            info[\"test_dir\"] = os.path.join(root_path, \"test\")\n",
        "\n",
        "    ext_counter = Counter()\n",
        "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
        "        for fname in filenames:\n",
        "            fpath = os.path.join(dirpath, fname)\n",
        "            info[\"num_files\"] += 1\n",
        "            ext = Path(fname).suffix.lstrip(\".\").lower()\n",
        "            ext_counter[ext] += 1\n",
        "            if is_image_file(fname):\n",
        "                info[\"types\"][\"images\"] += 1\n",
        "                if len(info[\"sample_files\"][\"images\"]) < max_preview_files:\n",
        "                    info[\"sample_files\"][\"images\"].append(fpath)\n",
        "            elif is_audio_file(fname):\n",
        "                info[\"types\"][\"audio\"] += 1\n",
        "                if len(info[\"sample_files\"][\"audio\"]) < max_preview_files:\n",
        "                    info[\"sample_files\"][\"audio\"].append(fpath)\n",
        "            elif is_text_file(fname):\n",
        "                info[\"types\"][\"text\"] += 1\n",
        "                if len(info[\"sample_files\"][\"text\"]) < max_preview_files:\n",
        "                    info[\"sample_files\"][\"text\"].append(fpath)\n",
        "            else:\n",
        "                info[\"types\"][\"others\"] += 1\n",
        "                if len(info[\"sample_files\"][\"others\"]) < max_preview_files:\n",
        "                    info[\"sample_files\"][\"others\"].append(fpath)\n",
        "\n",
        "    info[\"total_files_by_ext\"] = dict(ext_counter)\n",
        "\n",
        "    # detect class folders (immediate subdirs that contain many images)\n",
        "    immediate_dirs = [p for p in Path(root_path).iterdir() if p.is_dir()]\n",
        "    class_candidates = []\n",
        "    for d in immediate_dirs:\n",
        "        count_images = sum(1 for _ in d.rglob(\"*\") if is_image_file(_.name))\n",
        "        if count_images > 0:\n",
        "            class_candidates.append((d.name, count_images))\n",
        "    if len(class_candidates) > 1:\n",
        "        info[\"class_folders\"] = [c for c, n in class_candidates]\n",
        "    return info\n",
        "\n",
        "def plan_sampling(info, threshold=LARGE_DATASET_THRESHOLD):\n",
        "    plan = {\"use_sampling\": False, \"sample_percent\": 1.0, \"sample_count\": info.get(\"num_files\", 0)}\n",
        "    if info.get(\"num_files\", 0) > threshold:\n",
        "        plan[\"use_sampling\"] = True\n",
        "        plan[\"sample_percent\"] = 0.10\n",
        "        plan[\"sample_count\"] = max(1, int(round(info[\"num_files\"] * 0.10)))\n",
        "    return plan\n",
        "\n",
        "# ---------- Running generated Python code safely ----------\n",
        "\n",
        "def run_generated_python(code, workdir=\"/content\", timeout=1200):\n",
        "    \"\"\"\n",
        "    Runs generated Python code in a subprocess.\n",
        "    - Executes any '!pip install ...' lines first (in the current environment)\n",
        "    - Writes remaining code into a temp file under workdir and runs python on it\n",
        "    Returns (success:bool, stdout+stderr:str)\n",
        "    \"\"\"\n",
        "    os.makedirs(workdir, exist_ok=True)\n",
        "    pip_lines = []\n",
        "    other_lines = []\n",
        "    for line in code.splitlines():\n",
        "        stripped = line.strip()\n",
        "        if stripped.startswith(\"!pip\"):\n",
        "            pip_lines.append(stripped[1:].strip())\n",
        "        else:\n",
        "            other_lines.append(line)\n",
        "    try:\n",
        "        # Run pip installs first (if any)\n",
        "        for cmd in pip_lines:\n",
        "            print(f\"📦 Running install: {cmd}\")\n",
        "            subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        tmp_path = os.path.join(workdir, \"_gen_code.py\")\n",
        "        with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(other_lines))\n",
        "        # Run the script\n",
        "        proc = subprocess.run([\"python3\", tmp_path], cwd=workdir, capture_output=True, text=True, timeout=timeout)\n",
        "        stdout = proc.stdout or \"\"\n",
        "        stderr = proc.stderr or \"\"\n",
        "        success = proc.returncode == 0\n",
        "        return success, stdout + (\"\\n---STDERR---\\n\" + stderr if stderr else \"\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return False, f\"CalledProcessError: {e}\\n{getattr(e, 'stderr', '')}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Exception while running generated code:\\n{traceback.format_exc()}\"\n",
        "\n",
        "# ---------- Gemini helpers for iterative generation ----------\n",
        "\n",
        "def ask_gemini(prompt, temperature=0.0, max_output_chars=20000):\n",
        "    resp = MODEL.generate_content(prompt)\n",
        "    txt = safe_get_text(resp)\n",
        "    # trim if too long\n",
        "    if len(txt) > max_output_chars:\n",
        "        return txt[:max_output_chars]\n",
        "    return txt\n",
        "\n",
        "def iterative_generate_and_run(base_prompt, check_success_condition=None, max_attempts=5, workdir=\"/content\"):\n",
        "    \"\"\"\n",
        "    base_prompt -> ask Gemini to produce a Python script\n",
        "    Run it. On error, send both code and error to Gemini and ask for fix (repeat).\n",
        "    check_success_condition: optional function(success, combined_output) -> bool to accept success\n",
        "    Returns final_code, run_log, success_bool\n",
        "    \"\"\"\n",
        "    attempt = 1\n",
        "    prompt = base_prompt\n",
        "    last_code = None\n",
        "    run_log = \"\"\n",
        "    while attempt <= max_attempts:\n",
        "        print(f\"\\n--- Gemini generation attempt {attempt}/{max_attempts} ---\")\n",
        "        code = ask_gemini(prompt)\n",
        "        last_code = code\n",
        "        safe_write(os.path.join(workdir, f\"attempt_{attempt}_generated.py\"), code)\n",
        "        print(\">> Running generated code...\")\n",
        "        success, out = run_generated_python(code, workdir=workdir)\n",
        "        run_log += f\"\\n\\n--- Attempt {attempt} ---\\nSuccess: {success}\\nOutput:\\n{out}\\n\"\n",
        "        if success:\n",
        "            if check_success_condition is None or check_success_condition(True, out):\n",
        "                return code, run_log, True\n",
        "            else:\n",
        "                # consider as failure and provide more context\n",
        "                error_msg = f\"Script ran but check_success_condition failed.\\nOutput:\\n{out}\"\n",
        "        else:\n",
        "            error_msg = out\n",
        "            print(error_msg)\n",
        "        # prepare correction prompt\n",
        "        correction_prompt = f\"\"\"\n",
        "The following Python script was generated previously (the full script is below). When executed, it produced an error or didn't meet the required post-condition.\n",
        "\n",
        "Please return a corrected Python script (ONLY the Python code, no surrounding explanation) that fixes the problem.\n",
        "\n",
        "--- Previous script:\n",
        "{code}\n",
        "\n",
        "--- Observed problem / error / output:\n",
        "{error_msg}\n",
        "\n",
        "The environment is a Colab-like Linux environment. The script must be runnable as-is and must create/read any files under /content if necessary.\n",
        " Do NOT include any Markdown, backticks, or ```python fences.Return ONLY runnable Python code.\n",
        "\"\"\"\n",
        "        prompt = correction_prompt\n",
        "        attempt += 1\n",
        "    # if exhausted\n",
        "    return last_code, run_log, False\n",
        "\n",
        "# ---------- Top-level pipeline ----------\n",
        "\n",
        "def pipeline_from_pdf_and_zip(pdf_path, dataset_zip_path=None, extract_dir=DEFAULT_EXTRACT_DIR):\n",
        "    result = {\n",
        "        \"success\": False,\n",
        "        \"methodology_text\": None,\n",
        "        \"initial_dataset_info\": None,\n",
        "        \"preprocessing_code\": None,\n",
        "        \"preprocessing_run_log\": None,\n",
        "        \"final_dataset_info\": None,\n",
        "        \"model_code\": None,\n",
        "        \"model_run_log\": None,\n",
        "        \"artifacts\": {}\n",
        "    }\n",
        "\n",
        "    # 1) Extract paper text and methodology\n",
        "    print(\"📄 Extracting text from PDF...\")\n",
        "    paper_text = extract_text_from_pdf(pdf_path)\n",
        "    if not paper_text or len(paper_text) < 50:\n",
        "        print(\"⚠️ Warning: PDF text extraction returned very little content. Check PDF.\")\n",
        "    print(\"🧠 Asking Gemini to extract Methodology...\")\n",
        "    methodology_text = extract_methodology(paper_text)\n",
        "    methodology_text=\"\"\" \"\"\"\n",
        "    result[\"methodology_text\"] = methodology_text\n",
        "    safe_write(\"/content/extracted_methodology.txt\", methodology_text)\n",
        "    print(\"✅ Methodology extracted (saved to /content/extracted_methodology.txt)\")\n",
        "\n",
        "    # 2) If dataset zip provided: extract + scan\n",
        "    dataset_info = None\n",
        "    if dataset_zip_path:\n",
        "        print(\"📦 Extracting dataset zip...\")\n",
        "        dataset_root = extract_zip(dataset_zip_path, extract_to=extract_dir)\n",
        "        print(\"🔎 Scanning dataset...\")\n",
        "        dataset_info = scan_dataset(dataset_root)\n",
        "        result[\"initial_dataset_info\"] = dataset_info\n",
        "        safe_write(\"/content/initial_dataset_info.json\", json.dumps(dataset_info, indent=2))\n",
        "        print(f\"Found {dataset_info.get('num_files',0)} files. Types: {dataset_info.get('types')}\")\n",
        "    else:\n",
        "        print(\"ℹ️ No dataset zip provided. The pipeline will request Gemini to use dummy dataset if needed.\")\n",
        "        dataset_root = None\n",
        "\n",
        "    # 3) Prepare and run preprocessing generation (if dataset provided)\n",
        "    final_dataset_info = None\n",
        "    if dataset_root:\n",
        "        plan = plan_sampling(dataset_info)\n",
        "        sampling_msg = \"\"\n",
        "        if plan[\"use_sampling\"]:\n",
        "            sampling_msg = f\"Dataset is large (num_files={dataset_info['num_files']}). Use EXACTLY 10% sampling (sample_count={plan['sample_count']}) for train+test.\"\n",
        "        preprocess_prompt = f\"\"\"\n",
        "You are an expert ML engineer producing Colab-ready Python code for dataset preprocessing.\n",
        "Constraints:\n",
        "- Output only Python code (NO MARKDOWN, NO EXPLANATION).\n",
        "- The dataset root is: {dataset_root}\n",
        "- The goal: produce a Python script that:\n",
        "  1) Detects dataset structure (train/test folders, folder-per-class, csv listing, etc).\n",
        "  2) If train/test folders don't exist, create an 80/20 split.\n",
        "  3) Supports images, audio, and text/tabular CSV files.\n",
        "  4) If dataset is large, {sampling_msg}\n",
        "  5) Produces a JSON file at {PREPROCESS_OUTPUT_JSON} describing:\n",
        "      - final num files per split,\n",
        "      - paths used for train/test/val,\n",
        "      - detected classes (if any),\n",
        "      - summary of any transforms applied.\n",
        "  6) Saves processed/organized files under /content/processed/ as needed.\n",
        "  7) Keep runtime reasonable for Colab.\n",
        "\n",
        "Here is a quick scan of the dataset (you must use this info to make decisions):\n",
        "{json.dumps(dataset_info, indent=2)}\n",
        "\n",
        "Also the research methodology to inform preprocessing decisions (short):\n",
        "{methodology_text[:4000]}\n",
        "\n",
        "Return ONLY a runnable Python script that performs preprocessing and writes the JSON at {PREPROCESS_OUTPUT_JSON}. Include any !pip install lines at the top if necessary.\n",
        "\"\"\"\n",
        "        # iterative generation + run\n",
        "        print(\"🛠️ Generating preprocessing script via Gemini (iterative)...\")\n",
        "        code, run_log, ok = iterative_generate_and_run(preprocess_prompt, max_attempts=MAX_ATTEMPTS_PREPROCESS, workdir=\"/content\")\n",
        "        result[\"preprocessing_code\"] = code\n",
        "        result[\"preprocessing_run_log\"] = run_log\n",
        "        safe_write(PREPROCESSING_SCRIPT_PATH, code or \"\")\n",
        "        if not ok:\n",
        "            print(\"❌ Preprocessing generation failed after retries. Check logs.\")\n",
        "            # we still try to proceed using initial scan info\n",
        "            final_dataset_info = dataset_info\n",
        "        else:\n",
        "            # if success, try load PREPROCESS_OUTPUT_JSON\n",
        "            if os.path.exists(PREPROCESS_OUTPUT_JSON):\n",
        "                with open(PREPROCESS_OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "                    final_dataset_info = json.load(f)\n",
        "            else:\n",
        "                # fallback to re-scan\n",
        "                final_dataset_info = scan_dataset(dataset_root)\n",
        "            result[\"final_dataset_info\"] = final_dataset_info\n",
        "            safe_write(\"/content/final_dataset_info.json\", json.dumps(final_dataset_info, indent=2))\n",
        "            print(\"✅ Preprocessing finished. Final dataset info saved to /content/final_dataset_info.json\")\n",
        "    else:\n",
        "        print(\"ℹ️ Skipping preprocessing - no dataset provided.\")\n",
        "        final_dataset_info = None\n",
        "\n",
        "    # 4) Generate model+training code using final_dataset_info + methodology\n",
        "    print(\"🏗️ Generating model & training script via Gemini (iterative)...\")\n",
        "    dataset_info_str = json.dumps(final_dataset_info, indent=2) if final_dataset_info else \"None (use sklearn make_classification fallback)\"\n",
        "    model_prompt = f\"\"\"\n",
        "You are an expert ML engineer writing Colab-ready Python code for model definition, training and evaluation.\n",
        "Constraints:\n",
        "- Output ONLY Python code (NO explanations).Dont include ```python``` too in the output.\n",
        "- Training MUST run for EXACTLY 2 EPOCHS.\n",
        "- Save a training metadata JSON to {TRAINING_METADATA_JSON} with: epochs_run, final_accuracy, dataset_summary.\n",
        "- Save model artifact to /content/final_model.pt (or another file in /content).\n",
        "- Use random seed for determinism where applicable.\n",
        "- Use dataset paths & details from this dataset_info: {dataset_info_str}\n",
        "- If dataset_info is None, use sklearn.make_classification fallback dataset and still train 2 epochs.\n",
        "- Keep code runnable in Colab and include !pip installs if needed.\n",
        "\n",
        "Methodology (for model architecture/hyperparams) summary:\n",
        "{methodology_text[:6000]}\n",
        "\n",
        "Return ONLY a runnable Python script that trains the model for exactly 2 epochs and writes {TRAINING_METADATA_JSON}.\n",
        "\"\"\"\n",
        "    model_code, model_run_log, ok_model = iterative_generate_and_run(model_prompt, max_attempts=MAX_ATTEMPTS_MODEL, workdir=\"/content\")\n",
        "    result[\"model_code\"] = model_code\n",
        "    result[\"model_run_log\"] = model_run_log\n",
        "    safe_write(MODEL_SCRIPT_PATH, model_code or \"\")\n",
        "\n",
        "    # 5) Attempt to retrieve training metadata if produced\n",
        "    if os.path.exists(TRAINING_METADATA_JSON):\n",
        "        with open(TRAINING_METADATA_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "            try:\n",
        "                training_meta = json.load(f)\n",
        "                result[\"artifacts\"][\"training_metadata\"] = training_meta\n",
        "            except Exception:\n",
        "                result[\"artifacts\"][\"training_metadata_raw\"] = read_text(TRAINING_METADATA_JSON)\n",
        "    if os.path.exists(\"/content/final_model.pt\"):\n",
        "        result[\"artifacts\"][\"model_path\"] = \"/content/final_model.pt\"\n",
        "\n",
        "    result[\"success\"] = ok_model\n",
        "    print(\"🔚 Pipeline finished. success =\", result[\"success\"])\n",
        "    safe_write(\"/content/pipeline_result.json\", json.dumps(result, indent=2))\n",
        "    return result\n",
        "\n",
        "# ---------- Example usage ----------\n",
        "if __name__ == \"__main__\":\n",
        "    # Edit these:\n",
        "    pdf_path = \"/content/12911_2025_Article_3027.pdf\"         # <-- replace with your PDF path in Colab\n",
        "    dataset_zip_path = \"/content/dataset.zip\"    # <-- replace with uploaded dataset.zip or set to None\n",
        "\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"⚠️ PDF not found at {pdf_path}. Please upload it to /content or change pdf_path.\")\n",
        "    # dataset_zip_path optional:\n",
        "    if dataset_zip_path and not os.path.exists(dataset_zip_path):\n",
        "        print(f\"⚠️ Dataset zip not found at {dataset_zip_path}. Proceeding without dataset.\")\n",
        "        dataset_zip_path = None\n",
        "\n",
        "    res = pipeline_from_pdf_and_zip(pdf_path, dataset_zip_path)\n",
        "    print(\"\\n--- Pipeline summary ---\")\n",
        "    print(\"Methodology length:\", len(res.get(\"methodology_text\") or \"\"))\n",
        "    if res.get(\"final_dataset_info\"):\n",
        "        print(\"Final dataset files:\", res[\"final_dataset_info\"].get(\"num_files\"))\n",
        "    print(\"Preprocessing script saved to:\", PREPROCESSING_SCRIPT_PATH)\n",
        "    print(\"Model script saved to:\", MODEL_SCRIPT_PATH)\n",
        "    print(\"Result metadata saved to: /content/pipeline_result.json\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
